{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meeting Text Summarizer\n",
    "\n",
    "``` method = 'Extract' ``` \n",
    "Is a from-scratch extractive summarizer based off of TF-IDF some hard-coding for the transcription-specific stop words and filler words\n",
    "\n",
    "``` method = 'Abstract' ```\n",
    "Returns a abstractive summary using a pre-trained huggingface summarizer, default:\n",
    "\n",
    "https://huggingface.co/sshleifer/distilbart-cnn-12-6\n",
    "\n",
    "Because of the length of the transripts, the same Extractive method is used to condense the transript to the maximum token length of the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from summaryUtils import Summarizer\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "path = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smr = Summarizer(method = \"Extract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TipsForEngagingMeetings.txt\n",
    "# ProductPlanning.txt\n",
    "\n",
    "summary = smr.summarize(os.path.join(path, \"TipsForEngagingMeetings.txt\"), output_len=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentary\n",
    "\n",
    "* Extract: Implementing the tf-idf had the obvious quirks from spoken language, like fillers and the transript formatting.  The next step would probably to be doing some parts of speech and chunking to identify exactly what verb people are deciding to do about the meeting subject noun instead of just identifying what the most talkative person thinks about it.\n",
    "\n",
    "\n",
    "\n",
    "* Abstract: the HF wrapper obviously suffers from the small input length but does produce more human-readable text, albeit less coherent in meaning.  In this context, it's more of an idea about how one could streamline a transformer pipeline for large amounts of text.  A better idea might have been batching chunks of text to make a coherent summary of each chunk.\n",
    "\n",
    "\n",
    "\n",
    "* Other:  In looking into how to summarize text w/o a GPU at hand, I ran into this interesting paper about abusing the GPT3 API.  The point they seem to be making is that high-quality training data is more important than volume of training data for summaries and I suspect that's even moreso the case w/ meetings of a technial nature.\n",
    "\n",
    "https://arxiv.org/abs/2112.08674"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
